{
    "OpenAI GPT4o": {
        "model_name": "OpenAI GPT4o",
        "type": "closed source",
        "supported": true,
        "deployment_type": "remote",
        "parameters": {
            "seed": 7,
            "top_p": 0.9,
            "temperature": 0.5,
            "top_k": 30
        }
    },
    "OpenAI GPT4o mini": {
        "model_name": "OpenAI GPT4o mini",
        "type": "closed source",
        "supported": true,
        "deployment_type": "remote",
        "parameters": {
            "seed": 7,
            "top_p": 0.9,
            "temperature": 0.5,
            "top_k": 30
        }
    },
    "Qwen 2.5-coder (7B) 4bit": {
        "model_name": "Qwen 2.5-coder (7B) 4bit",
        "type": "open source",
        "supported": true,
        "ollama_command": "qwen2.5-coder:7b-instruct-q4_K_S",
        "vram_requirement": "5GB",
        "deployment_type": "local",
        "parameters": {
            "seed": 7,
            "num_predict": -1,
            "num_ctx": 32768,
            "top_k": 50,
            "top_p": 0.8,
            "temperature": 0.5
        }
    },
    "Qwen 2.5-coder (7B) fp": {
        "model_name": "Qwen 2.5-coder (7B) fp",
        "type": "open source",
        "supported": true,
        "ollama_command": "qwen2.5-coder:7b-instruct-fp16",
        "vram_requirement": "16GB",
        "deployment_type": "local",
        "parameters": {
            "seed": 7,
            "num_predict": -1,
            "num_ctx": 32768,
            "top_k": 50,
            "top_p": 0.8,
            "temperature": 0
        }
    },
    "Qwen 2.5-coder (32B) fp": {
        "model_name": "Qwen 2.5-coder (32B) fp",
        "type": "open source",
        "supported": true,
        "ollama_command": "qwen2.5-coder:32b-instruct-fp16",
        "vram_requirement": "66GB",
        "deployment_type": "local",
        "parameters": {
            "seed": 7,
            "num_predict": -1,
            "num_ctx": 32768,
            "top_k": 50,
            "top_p": 0.8,
            "temperature": 0
        }
    },
    "Qwen 2.5 (32B) fp": {
        "model_name": "Qwen 2.5 (32B) fp",
        "supported": true,
        "ollama_command": "qwen2.5:32b-instruct-fp16",
        "vram_requirement": "66GB",
        "deployment_type": "local",
        "parameters": {
            "seed": 7,
            "num_predict": -1,
            "num_ctx": 128000,
            "top_k": 50,
            "top_p": 0.8,
            "temperature": 0
        }
    },
    "Qwen 2.5 (72B) 4bit": {
        "model_name": "Qwen 2.5 (72B) 4bit",
        "type": "open source",
        "supported": true,
        "ollama_command": "qwen2.5:72b-instruct-q4_K_M",
        "vram_requirement": "47GB",
        "deployment_type": "local",
        "parameters": {
            "seed": 7,
            "num_predict": -1,
            "num_ctx": 128000,
            "top_k": 50,
            "top_p": 0.8,
            "temperature": 0.5
        }
    },
    "Qwen 2.5 (72B) 8bit": {
        "model_name": "Qwen 2.5 (72B) 8bit",
        "type": "open source",
        "supported": true,
        "ollama_command": "qwen2.5:72b-instruct-q8_0",
        "vram_requirement": "77GB",
        "deployment_type": "local",
        "parameters": {
            "seed": 7,
            "num_predict": -1,
            "num_ctx": 128000,
            "top_k": 50,
            "top_p": 0.8,
            "temperature": 0.5
        }
    },
    "Codellama (7B) 4bit": {
        "model_name": "Codellama (7B) 4bit",
        "type": "open source",
        "supported": true,
        "ollama_command": "codellama:7b-instruct-q4_K_M",
        "vram_requirement": "5GB",
        "deployment_type": "local",
        "parameters": {
            "seed": 7,
            "num_predict": -1,
            "num_ctx": 16384,
            "top_k": 50,
            "top_p": 0.8,
            "temperature": 0.5
        }
    },
    "Codellama (7B) fp": {
        "model_name": "Codellama (7B) fp",
        "type": "open source",
        "supported": true,
        "ollama_command": "codellama:7b-instruct-fp16",
        "vram_requirement": "16GB",
        "deployment_type": "local",
        "parameters": {
            "seed": 7,
            "num_predict": -1,
            "num_ctx": 16000,
            "top_k": 50,
            "top_p": 0.8,
            "temperature": 0.5
        }
    },
    "Codellama (13B) fp": {
        "model_name": "Codellama (13B) fp",
        "type": "open source",
        "supported": true,
        "ollama_command": "codellama:13b-instruct-fp16",
        "vram_requirement": "16GB",
        "deployment_type": "local",
        "parameters": {
            "seed": 7,
            "num_predict": -1,
            "num_ctx": 16000,
            "top_k": 30,
            "top_p": 0.9,
            "temperature": 0.5
        }
    },
    "Codellama (34B) fp": {
        "model_name": "Codellama (34B) fp",
        "type": "open source",
        "supported": true,
        "ollama_command": "codellama:34b-instruct-fp16",
        "vram_requirement": "67GB",
        "deployment_type": "local",
        "parameters": {
            "seed": 7,
            "num_predict": -1,
            "num_ctx": 16000,
            "top_k": 50,
            "top_p": 0.8,
            "temperature": 0.5
        }
    },
    "Codellama (70B) fp": {
        "model_name": "Codellama (70B) fp",
        "type": "open source",
        "supported": true,
        "ollama_command": "codellama:70b-instruct-fp16",
        "vram_requirement": "138GB",
        "deployment_type": "local",
        "parameters": {
            "seed": 7,
            "num_predict": -1,
            "num_ctx": 16000,
            "top_k": 50,
            "top_p": 0.8,
            "temperature": 0.5
        }
    },
    "Llama3.1 (8B) 4bit": {
        "model_name": "Llama3.1 (8B) 4bit",
        "type": "open source",
        "supported": true,
        "ollama_command": "llama3.1:8b-instruct-q4_K_M",
        "vram_requirement": "5GB",
        "deployment_type": "local",
        "parameters": {
            "seed": 7,
            "num_predict": -1,
            "num_ctx": 132000,
            "top_k": 50,
            "top_p": 0.8,
            "temperature": 0.5
        }
    },
    "LLaMA 3.1 (8B) fp": {
        "model_name": "LLaMA 3.1 (8B) fp",
        "type": "open source",
        "supported": true,
        "ollama_command": "llama3.1:8b-instruct-fp16",
        "vram_requirement": "16GB",
        "deployment_type": "local",
        "parameters": {
            "seed": 7,
            "num_predict": -1,
            "num_ctx": 132000,
            "top_k": 50,
            "top_p": 0.8,
            "temperature": 0.5
        }
    },
    "LLaMA 3.1 (70B) 4bit": {
        "model_name": "LLaMA 3.1 (70B) 4bit",
        "type": "open source",
        "supported": true,
        "ollama_command": "llama3.1:70b-instruct-q4_K_M",
        "vram_requirement": "50GB",
        "quantization": "4-bit",
        "deployment_type": "local",
        "parameters": {
            "seed": 7,
            "num_predict": -1,
            "num_ctx": 132000,
            "top_k": 50,
            "top_p": 0.8,
            "temperature": 0.5
        }
    },
    "LLaMA 3.1 (70B) 8bit": {
        "model_name": "LLaMA 3.1 (70B) 8bit",
        "type": "open source",
        "supported": true,
        "ollama_command": "llama3.1:70b-instruct-q8_0",
        "vram_requirement": "75GB",
        "quantization": "8-bit",
        "deployment_type": "local",
        "parameters": {
            "seed": 7,
            "num_predict": -1,
            "num_ctx": 132000,
            "top_k": 50,
            "top_p": 0.8,
            "temperature": 0.5
        }
    },
    "LLaMa 3.3 (70B) fp": {
        "model_name": "LLaMa 3.3 (70B) fp",
        "type": "open source",
        "supported": true,
        "ollama_command": "llama3.3:70b-instruct-fp16",
        "vram_requirement": "141GB",
        "deployment_type": "local",
        "parameters": {
            "seed": 7,
            "num_predict": -1,
            "num_ctx": 8000,
            "top_k": 50,
            "top_p": 0.8,
            "temperature": 0.5
        }
    },
    "StarCoder2 (3B) fp": {
        "model_name": "StarCoder2 (3B) fp",
        "type": "open source",
        "supported": false,
        "ollama_command": "starcoder2:3b-fp16",
        "vram_requirement": "6.1GB",
        "deployment_type": "local",
        "parameters": {
            "seed": 7,
            "num_predict": -1,
            "num_ctx": 16384,
            "top_k": 50,
            "top_p": 0.8,
            "temperature": 0.5
        }
    },
    "StarCoder2 (7B) fp": {
        "model_name": "StarCoder2 (7B) fp",
        "type": "open source",
        "supported": false,
        "ollama_command": "starcoder2:7b-fp16",
        "vram_requirement": "14GB",
        "deployment_type": "local",
        "parameters": {
            "seed": 7,
            "num_predict": -1,
            "num_ctx": 16384,
            "top_k": 30,
            "top_p": 0.9,
            "temperature": 0.5
        }
    },
    "StarCoder2 (15B) fp": {
        "model_name": "StarCoder2 (15B) fp",
        "type": "open source",
        "supported": true,
        "ollama_command": "starcoder2:15b-instruct-v0.1-fp16",
        "vram_requirement": "32GB",
        "deployment_type": "local",
        "parameters": {
            "seed": 7,
            "num_predict": -1,
            "num_ctx": 16384,
            "top_k": 50,
            "top_p": 0.8,
            "temperature": 0.5
        }
    },
    "Mistral (7B) fp": {
        "model_name": "Mistral (7B) fp",
        "type": "open source",
        "supported": true,
        "ollama_command": "mistral:7b-instruct-fp16",
        "vram_requirement": "14GB",
        "deployment_type": "local",
        "parameters": {
            "seed": 7,
            "num_predict": -1,
            "num_ctx": 32768,
            "top_k": 50,
            "top_p": 0.8,
            "temperature": 0.5
        }
    },
    "Nemotron 70B fp": {
        "model_name": "Nemotron 70B fp",
        "type": "open source",
        "supported": true,
        "ollama_command": "nemotron:70b-instruct-fp16",
        "vram_requirement": "71GB",
        "deployment_type": "local",
        "parameters": {
            "seed": 7,
            "num_predict": -1,
            "num_ctx": 131072,
            "top_k": 50,
            "top_p": 0.8,
            "temperature": 0.5
        }
    },
    "Mistral-large2 (123B) 6bit": {
        "model_name": "Mistral-large2 (123B) 6bit",
        "type": "open source",
        "supported": true,
        "ollama_command": "mistral-large:123b-instruct-2407-q6_K",
        "vram_requirement": "101GB",
        "deployment_type": "local",
        "parameters": {
            "seed": 7,
            "num_predict": -1,
            "num_ctx": 131072,
            "top_k": 50,
            "top_p": 0.8,
            "temperature": 0.5
        }
    },
    "Mistral-large2 (123B) 8bit": {
        "model_name": "Mistral-large2 (123B) 8bit",
        "type": "open source",
        "supported": true,
        "ollama_command": "mistral-large:123b-instruct-2407-q8_0",
        "vram_requirement": "130GB",
        "deployment_type": "local",
        "parameters": {
            "seed": 7,
            "num_predict": -1,
            "num_ctx": 131072,
            "top_k": 50,
            "top_p": 0.8,
            "temperature": 0.5
        }
    },
    "Mistral Nemo (12B) fp": {
        "model_name": "Mistral Nemo (12B) fp",
        "type": "open source",
        "supported": true,
        "ollama_command": "mistral-nemo:12b-instruct-2407-fp16",
        "vram_requirement": "25GB",
        "deployment_type": "local",
        "parameters": {
            "seed": 7,
            "num_predict": -1,
            "num_ctx": 1024000,
            "top_k": 50,
            "top_p": 0.8,
            "temperature": 0.5
        }
    },
    "qwq (32B) 4bit": {
        "model_name": "qwq (32B) 4bit",
        "type": "open source",
        "supported": true,
        "ollama_command": "qwq:32b-preview-q4_K_M",
        "vram_requirement": "20GB",
        "deployment_type": "local",
        "parameters": {
            "seed": 7,
            "num_predict": -1,
            "num_ctx": 32768,
            "top_k": 50,
            "top_p": 0.8,
            "temperature": 0
        }
    },
    "qwq (32B) 8bit": {
        "model_name": "qwq (32B) 8bit",
        "type": "open source",
        "supported": true,
        "ollama_command": "qwq:32b-preview-q8_0",
        "vram_requirement": "35GB",
        "deployment_type": "local",
        "parameters": {
            "seed": 7,
            "num_predict": -1,
            "num_ctx": 32768,
            "top_k": 50,
            "top_p": 0.8,
            "temperature": 0.5
        }
    },
    "qwq (32B) fp": {
        "model_name": "qwq (32B) fp",
        "type": "open source",
        "supported": true,
        "ollama_command": "qwq:32b-preview-fp16",
        "vram_requirement": "66GB",
        "deployment_type": "local",
        "parameters": {
            "seed": 7,
            "num_predict": -1,
            "num_ctx": 32768,
            "top_k": 50,
            "top_p": 0.8,
            "temperature": 0
        }
    },
    "Falcon3 (10B) fp": {
        "model_name": "Falcon3 (10B) fp",
        "type": "open source",
        "supported": true,
        "ollama_command": "falcon3:10b-instruct-fp16",
        "vram_requirement": "21GB",
        "deployment_type": "local",
        "parameters": {
            "seed": 7,
            "num_predict": -1,
            "num_ctx": 32768,
            "top_k": 50,
            "top_p": 0.8,
            "temperature": 0.5
        }
    },
    "Falcon3 (7B) fp": {
        "model_name": "Falcon3 (7B) fp",
        "type": "open source",
        "supported": true,
        "ollama_command": "falcon3:7b-instruct-fp16",
        "vram_requirement": "15GB",
        "deployment_type": "local",
        "parameters": {
            "seed": 7,
            "num_predict": -1,
            "num_ctx": 32768,
            "top_k": 50,
            "top_p": 0.8,
            "temperature": 0.5
        }
    },
    "Deepseek r1 (32B) fp": {
        "model_name": "Deepseek r1 (32B) fp",
        "type": "open source",
        "supported": true,
        "ollama_command": "deepseek-r1:32b-qwen-distill-fp16",
        "vram_requirement": "66GB",
        "deployment_type": "local",
        "parameters": {
            "seed": 7,
            "num_predict": -1,
            "num_ctx": 16384,
            "top_k": 30,
            "top_p": 0.9,
            "temperature": 0.5
        }
    },
    "Deepseek r1 (70B) fp": {
        "model_name": "Deepseek r1 (70B) fp",
        "type": "open source",
        "supported": true,
        "ollama_command": "deepseek-r1:70b-llama-distill-fp16",
        "vram_requirement": "141GB",
        "deployment_type": "local",
        "parameters": {
            "seed": 7,
            "num_predict": -1,
            "num_ctx": 16384,
            "top_k": 30,
            "top_p": 0.9,
            "temperature": 0.5
        }
    },
    "Deepseek-coder-v2 (16B) fp": {
        "model_name": "Deepseek-coder-v2 (16B) fp",
        "type": "open source",
        "supported": true,
        "ollama_command": "deepseek-coder-v2:16b-lite-instruct-fp16",
        "vram_requirement": "31GB",
        "deployment_type": "local",
        "parameters": {
            "seed": 7,
            "num_predict": -1,
            "num_ctx": 16384,
            "top_k": 50,
            "top_p": 0.8,
            "temperature": 0
        }
    },
    "Deepseek-coder-v2 (236B) 4bit": {
        "model_name": "Deepseek-coder-v2 (236B) 4bit",
        "type": "open source",
        "supported": true,
        "ollama_command": "deepseek-coder-v2:236b-instruct-q4_1",
        "vram_requirement": "148GB",
        "deployment_type": "local",
        "parameters": {
            "seed": 7,
            "num_predict": -1,
            "num_ctx": 16384,
            "top_k": 50,
            "top_p": 0.8,
            "temperature": 0
        }
    }
}