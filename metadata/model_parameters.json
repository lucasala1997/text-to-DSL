{
    "GPT-J (6B)": {
        "model_name": "GPT-J (6B)",
        "supported": "False",
        "vram_requirement": "24GB",
        "precision": "full",
        "quantization": null,
        "deployment_type": "local",
        "temperature": 0.7,
        "max_tokens": 256,
        "top_p": 0.9,
        "frequency_penalty": 0.0,
        "presence_penalty": 0.0,
        "top_k": 50
    },
    "GPT-NeoX (20B)": {
        "model_name": "GPT-NeoX (20B)",
        "supported": "False",
        "vram_requirement": "24GB",
        "precision": "full",
        "quantization": null,
        "deployment_type": "local",
        "temperature": 0.7,
        "max_tokens": 256,
        "top_p": 0.9,
        "frequency_penalty": 0.0,
        "presence_penalty": 0.0,
        "top_k": 50
    },
    "CodeGen-2 (6B)": {
        "model_name": "CodeGen-2 (6B)",
        "supported": "False",
        "vram_requirement": "24GB",
        "precision": "full",
        "quantization": null,
        "deployment_type": "local",
        "temperature": 0.7,
        "max_tokens": 256,
        "top_p": 0.9,
        "frequency_penalty": 0.0,
        "presence_penalty": 0.0,
        "top_k": 50
    },
    "LLaMA 3.1 (8B)": {
        "model_name": "LLaMA 3.1 (8B)",
        "supported": "True",
        "ollama_command": "ollama run vanilj/llama-3-14b-instruct-v1:Q8_0",
        "vram_requirement": "24GB",
        "precision": "full",
        "quantization": null,
        "deployment_type": "local",
        "temperature": 0.7,
        "max_tokens": 256,
        "top_p": 0.9,
        "frequency_penalty": 0.0,
        "presence_penalty": 0.0,
        "top_k": 50
    },
    "PolyCoder": {
        "model_name": "PolyCoder",
        "supported": "False",
        "vram_requirement": "24GB",
        "precision": "full",
        "quantization": null,
        "deployment_type": "local",
        "temperature": 0.7,
        "max_tokens": 256,
        "top_p": 0.9,
        "frequency_penalty": 0.0,
        "presence_penalty": 0.0,
        "top_k": 50
    },
    "Qwen 2.5-code": {
        "model_name": "Qwen 2.5-code",
        "supported": "False",
        "vram_requirement": "48GB",
        "precision": "full",
        "quantization": null,
        "deployment_type": "local",
        "temperature": 0.7,
        "max_tokens": 256,
        "top_p": 0.9,
        "frequency_penalty": 0.0,
        "presence_penalty": 0.0,
        "top_k": 50
    },
    "StarCoder": {
        "model_name": "StarCoder",
        "supported": "False",
        "vram_requirement": "48GB",
        "precision": "full",
        "quantization": null,
        "deployment_type": "local",
        "temperature": 0.7,
        "max_tokens": 256,
        "top_p": 0.9,
        "frequency_penalty": 0.0,
        "presence_penalty": 0.0,
        "top_k": 50
    },
    "LLaMA 3.1 (70B quantized)": {
        "model_name": "LLaMA 3.1 (70B)",
        "supported": "False",
        "vram_requirement": "48GB",
        "precision": "quantized",
        "quantization": "4-bit",
        "deployment_type": "local",
        "temperature": 0.7,
        "max_tokens": 256,
        "top_p": 0.9,
        "frequency_penalty": 0.0,
        "presence_penalty": 0.0,
        "top_k": 50
    },
    "CodeGen (16B)": {
        "model_name": "CodeGen (16B)",
        "supported": "False",
        "vram_requirement": "96GB",
        "precision": "full",
        "quantization": null,
        "deployment_type": "local",
        "temperature": 0.7,
        "max_tokens": 256,
        "top_p": 0.9,
        "frequency_penalty": 0.0,
        "presence_penalty": 0.0,
        "top_k": 50
    },
    "LLaMA-2 (70B)": {
        "model_name": "LLaMA-2 (70B)",
        "supported": "False",
        "vram_requirement": "96GB",
        "precision": "full",
        "quantization": null,
        "deployment_type": "local",
        "temperature": 0.7,
        "max_tokens": 256,
        "top_p": 0.9,
        "frequency_penalty": 0.0,
        "presence_penalty": 0.0,
        "top_k": 50
    },
    "PaLM (62B)": {
        "model_name": "PaLM (62B)",
        "supported": "False",
        "vram_requirement": "128GB",
        "precision": "full",
        "quantization": null,
        "deployment_type": "local",
        "temperature": 0.7,
        "max_tokens": 256,
        "top_p": 0.9,
        "frequency_penalty": 0.0,
        "presence_penalty": 0.0,
        "top_k": 50
    }
}